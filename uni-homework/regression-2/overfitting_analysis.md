# âš ï¸ Overfitting Analizi ve Ã‡Ã¶zÃ¼m Ã–nerileri

## ğŸ” Overfitting Belirtileri

### ğŸ“Š Train vs Test Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Model | Train RÂ² | Test RÂ² | RÂ² FarkÄ± | Train RMSE | Test RMSE | RMSE OranÄ± (Test/Train) |
|-------|----------|---------|----------|------------|-----------|-------------------------|
| **Random Forest** | 0.9999 | 0.9997 | 0.0002 | 0.031 | 0.073 | **2.35x** âš ï¸ |
| **Gradient Boosting** | 0.9999 | 0.9997 | 0.0002 | 0.048 | 0.081 | **1.69x** âš ï¸ |
| **XGBoost** | 0.9995 | 0.9986 | 0.0009 | 0.100 | 0.161 | **1.61x** âš ï¸ |
| **LightGBM** | 0.9994 | 0.9994 | 0.0000 | 0.105 | 0.109 | **1.04x** âœ… |
| **Ridge Regression** | 0.9532 | 0.9539 | -0.0007 | 0.944 | 0.941 | **1.00x** âœ… |
| **Linear Regression** | 0.9532 | 0.9539 | -0.0007 | 0.944 | 0.941 | **1.00x** âœ… |

### ğŸš¨ Overfitting Ä°ÅŸaretleri

1. **Random Forest**: 
   - Test RMSE, Train RMSE'den **2.35 kat** daha yÃ¼ksek
   - Train RÂ² = 0.9999 (neredeyse mÃ¼kemmel)
   - Test RÂ² = 0.9997 (hala Ã§ok yÃ¼ksek ama train'den dÃ¼ÅŸÃ¼k)

2. **Gradient Boosting**:
   - Test RMSE, Train RMSE'den **1.69 kat** daha yÃ¼ksek
   - Train RÂ² = 0.9999
   - Test RÂ² = 0.9997

3. **XGBoost**:
   - Test RMSE, Train RMSE'den **1.61 kat** daha yÃ¼ksek
   - Train RÂ² = 0.9995
   - Test RÂ² = 0.9986

### âœ… Overfitting Olmayan Modeller

- **LightGBM**: Test/Train RMSE oranÄ± 1.04x (Ã§ok iyi)
- **Ridge/Linear Regression**: Test/Train RMSE oranÄ± ~1.00x (mÃ¼kemmel)

---

## ğŸ¯ Overfitting Nedenleri

### 1. **Model KompleksliÄŸi Ã‡ok YÃ¼ksek**
- Random Forest ve Gradient Boosting modelleri Ã§ok derin aÄŸaÃ§lar kullanÄ±yor olabilir
- Ã‡ok fazla aÄŸaÃ§ sayÄ±sÄ± (n_estimators) overfitting'e yol aÃ§abilir

### 2. **Ã–zellik SayÄ±sÄ± Fazla Olabilir**
- GeniÅŸletilmiÅŸ Ã¶zellik mÃ¼hendisliÄŸi ile Ã§ok fazla Ã¶zellik oluÅŸturulmuÅŸ olabilir
- BazÄ± Ã¶zellikler gÃ¼rÃ¼ltÃ¼ iÃ§erebilir

### 3. **Veri Seti Boyutu**
- %3 veri kullanÄ±lÄ±yor (deneme amaÃ§lÄ±)
- KÃ¼Ã§Ã¼k veri seti Ã¼zerinde kompleks modeller overfitting'e yatkÄ±n

### 4. **Hiperparametre Optimizasyonu**
- RandomizedSearchCV ile sÄ±nÄ±rlÄ± iterasyon (n_iter=6)
- Daha iyi hiperparametreler bulunabilir

---

## ğŸ’¡ Ã‡Ã¶zÃ¼m Ã–nerileri

### 1. **Model KompleksliÄŸini Azaltma**

#### Random Forest iÃ§in:
```python
RandomForestRegressor(
    n_estimators=50,        # 100'den azalt
    max_depth=10,           # Daha sÄ±ÄŸ aÄŸaÃ§lar
    min_samples_split=20,   # Daha fazla Ã¶rnek gerektir
    min_samples_leaf=10,    # Yaprak dÃ¼ÄŸÃ¼mlerinde daha fazla Ã¶rnek
    max_features='sqrt',    # Ã–zellik sayÄ±sÄ±nÄ± sÄ±nÄ±rla
    random_state=42
)
```

#### Gradient Boosting iÃ§in:
```python
GradientBoostingRegressor(
    n_estimators=50,        # 100'den azalt
    max_depth=5,            # Daha sÄ±ÄŸ aÄŸaÃ§lar
    learning_rate=0.05,     # Daha dÃ¼ÅŸÃ¼k learning rate
    min_samples_split=20,
    min_samples_leaf=10,
    subsample=0.8,         # Her iterasyonda %80 veri kullan
    random_state=42
)
```

#### XGBoost iÃ§in:
```python
XGBRegressor(
    n_estimators=50,
    max_depth=5,
    learning_rate=0.05,
    min_child_weight=5,
    subsample=0.8,
    colsample_bytree=0.8,  # Ã–zellik Ã¶rnekleme
    reg_alpha=0.1,         # L1 regularization
    reg_lambda=1.0,        # L2 regularization
    random_state=42
)
```

### 2. **Early Stopping Kullanma**

```python
# XGBoost ve LightGBM iÃ§in early stopping
XGBRegressor(
    ...
    early_stopping_rounds=10,
    eval_set=[(X_test, y_test)]
)
```

### 3. **Cross-Validation ile Model SeÃ§imi**

```python
from sklearn.model_selection import cross_val_score

# Her model iÃ§in CV skorlarÄ±nÄ± karÅŸÄ±laÅŸtÄ±r
cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
print(f"CV RMSE: {-cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
```

### 4. **Ã–zellik SeÃ§imi**

```python
from sklearn.feature_selection import SelectKBest, f_regression

# En Ã¶nemli Ã¶zellikleri seÃ§
selector = SelectKBest(f_regression, k=20)  # En iyi 20 Ã¶zellik
X_train_selected = selector.fit_transform(X_train, y_train)
X_test_selected = selector.transform(X_test)
```

### 5. **Daha Fazla Veri Kullanma**

- %3 yerine %10-20 veri kullanÄ±labilir
- Daha fazla veri overfitting'i azaltÄ±r

### 6. **Regularization ArtÄ±rma**

```python
# Ridge Regression iÃ§in alpha deÄŸerini artÄ±r
Ridge(alpha=10.0)  # VarsayÄ±lan 1.0'dan daha yÃ¼ksek

# Lasso Regression iÃ§in alpha deÄŸerini artÄ±r
Lasso(alpha=1.0)   # VarsayÄ±lan 0.1'den daha yÃ¼ksek
```

### 7. **Ensemble YÃ¶ntemleri**

```python
from sklearn.ensemble import VotingRegressor

# Daha basit modelleri birleÅŸtir
voting_model = VotingRegressor([
    ('rf', RandomForestRegressor(n_estimators=50, max_depth=10)),
    ('gb', GradientBoostingRegressor(n_estimators=50, max_depth=5)),
    ('xgb', XGBRegressor(n_estimators=50, max_depth=5))
])
```

---

## ğŸ“ˆ Ã–nerilen Model AyarlarÄ±

### Random Forest (Overfitting AzaltÄ±lmÄ±ÅŸ)
```python
RandomForestRegressor(
    n_estimators=50,
    max_depth=10,
    min_samples_split=20,
    min_samples_leaf=10,
    max_features='sqrt',
    random_state=42,
    n_jobs=-1
)
```

### Gradient Boosting (Overfitting AzaltÄ±lmÄ±ÅŸ)
```python
GradientBoostingRegressor(
    n_estimators=50,
    max_depth=5,
    learning_rate=0.05,
    min_samples_split=20,
    min_samples_leaf=10,
    subsample=0.8,
    random_state=42
)
```

### XGBoost (Overfitting AzaltÄ±lmÄ±ÅŸ)
```python
XGBRegressor(
    n_estimators=50,
    max_depth=5,
    learning_rate=0.05,
    min_child_weight=5,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=1.0,
    random_state=42,
    n_jobs=-1
)
```

---

## ğŸ¯ SonuÃ§ ve Ã–neriler

### âš ï¸ Overfitting Var mÄ±?

**Evet**, Ã¶zellikle Random Forest ve Gradient Boosting modellerinde overfitting belirtileri var:
- Test RMSE, Train RMSE'den 1.6-2.3 kat daha yÃ¼ksek
- Train skorlarÄ± Ã§ok yÃ¼ksek (0.9999)

### âœ… Ne YapmalÄ±?

1. **KÄ±sa Vadede**: Model kompleksliÄŸini azalt (max_depth, n_estimators)
2. **Orta Vadede**: Daha fazla veri kullan (%10-20)
3. **Uzun Vadede**: Ã–zellik seÃ§imi ve daha iyi hiperparametre optimizasyonu

### ğŸ“Š Beklenen SonuÃ§lar

Overfitting azaltÄ±ldÄ±ktan sonra:
- Train RÂ²: 0.9999 â†’ 0.998-0.999 (biraz dÃ¼ÅŸecek)
- Test RÂ²: 0.9997 â†’ 0.998-0.999 (aynÄ± kalacak veya artacak)
- Test/Train RMSE OranÄ±: 2.35x â†’ 1.1-1.2x (daha dengeli)

---

**Not**: Overfitting her zaman kÃ¶tÃ¼ deÄŸildir. EÄŸer test performansÄ± hala Ã§ok iyiyse (RÂ² > 0.99), model kullanÄ±labilir. Ancak production'da beklenmedik veriler geldiÄŸinde performans dÃ¼ÅŸebilir.

